#!/usr/bin/env python3
"""
BPEè®­ç»ƒæ€§èƒ½å¯¹æ¯”è„šæœ¬
æ¯”è¾ƒåŸå§‹ç‰ˆæœ¬å’Œä¼˜åŒ–ç‰ˆæœ¬çš„æ€§èƒ½å·®å¼‚
"""

import time
import tempfile
from pathlib import Path
from train_bpe_with_progress import train_bpe_with_dataset
from train_bpe_optimized import train_bpe_optimized


def create_test_data(size_mb: int = 10) -> str:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    # åˆ›å»ºåŒ…å«å¤šä¸ªæ–‡æ¡£çš„æµ‹è¯•æ–‡æœ¬
    base_text = """This is a sample document for testing BPE performance.
It contains various words and patterns that will be used for tokenization.
The quick brown fox jumps over the lazy dog.
Machine learning and natural language processing are fascinating fields.
<|endoftext|>
Another document with different content and vocabulary.
Deep learning models require large amounts of training data.
Transformers have revolutionized the field of NLP.
Attention mechanisms allow models to focus on relevant parts of the input.
<|endoftext|>
Final document with numbers and special characters: 123, 456, 789!
@#$%^&*() symbols and punctuation marks.
This helps test the robustness of the tokenizer.
<|endoftext|>"""
    
    # é‡å¤æ–‡æœ¬ä»¥è¾¾åˆ°æŒ‡å®šå¤§å°
    target_size = size_mb * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
    repeated_text = base_text * (target_size // len(base_text) + 1)
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:
        f.write(repeated_text)
        return f.name


def compare_performance(test_file: str, vocab_size: int = 1000):
    """æ¯”è¾ƒä¸¤ä¸ªç‰ˆæœ¬çš„æ€§èƒ½"""
    print("=== BPEè®­ç»ƒæ€§èƒ½å¯¹æ¯” ===\n")
    
    print(f"æµ‹è¯•é…ç½®:")
    print(f"  æµ‹è¯•æ–‡ä»¶: {test_file}")
    print(f"  æ–‡ä»¶å¤§å°: {Path(test_file).stat().st_size / (1024*1024):.2f} MB")
    print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print()
    
    # æµ‹è¯•åŸå§‹ç‰ˆæœ¬
    print("ğŸ”„ æµ‹è¯•åŸå§‹ç‰ˆæœ¬...")
    start_time = time.time()
    
    try:
        results_original = train_bpe_with_dataset(
            input_path=test_file,
            vocab_size=vocab_size,
            special_tokens=["<|endoftext|>"]
        )
        original_time = time.time() - start_time
        
        if results_original:
            print(f"âœ… åŸå§‹ç‰ˆæœ¬å®Œæˆ: {original_time:.2f} ç§’")
            print(f"   è¯æ±‡è¡¨å¤§å°: {len(results_original['vocab'])}")
            print(f"   åˆå¹¶è§„åˆ™æ•°: {len(results_original['merges'])}")
        else:
            print("âŒ åŸå§‹ç‰ˆæœ¬å¤±è´¥")
            original_time = None
    except Exception as e:
        print(f"âŒ åŸå§‹ç‰ˆæœ¬å¤±è´¥: {e}")
        original_time = None
    
    print()
    
    # æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬
    print("ğŸš€ æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬...")
    start_time = time.time()
    
    try:
        results_optimized = train_bpe_optimized(
            input_path=test_file,
            vocab_size=vocab_size,
            special_tokens=["<|endoftext|>"]
        )
        optimized_time = time.time() - start_time
        
        if results_optimized:
            print(f"âœ… ä¼˜åŒ–ç‰ˆæœ¬å®Œæˆ: {optimized_time:.2f} ç§’")
            print(f"   è¯æ±‡è¡¨å¤§å°: {len(results_optimized['vocab'])}")
            print(f"   åˆå¹¶è§„åˆ™æ•°: {len(results_optimized['merges'])}")
        else:
            print("âŒ ä¼˜åŒ–ç‰ˆæœ¬å¤±è´¥")
            optimized_time = None
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–ç‰ˆæœ¬å¤±è´¥: {e}")
        optimized_time = None
    
    print()
    
    # æ€§èƒ½æ¯”è¾ƒ
    if original_time and optimized_time:
        speedup = original_time / optimized_time
        time_saved = original_time - optimized_time
        improvement_percent = (time_saved / original_time) * 100
        
        print(f"ğŸ“Š æ€§èƒ½æ¯”è¾ƒç»“æœ:")
        print(f"   åŸå§‹ç‰ˆæœ¬: {original_time:.2f} ç§’")
        print(f"   ä¼˜åŒ–ç‰ˆæœ¬: {optimized_time:.2f} ç§’")
        print(f"   åŠ é€Ÿæ¯”: {speedup:.2f}x")
        print(f"   æ—¶é—´èŠ‚çœ: {time_saved:.2f} ç§’ ({improvement_percent:.1f}%)")
        
        # æ£€æŸ¥ç»“æœä¸€è‡´æ€§
        if results_original and results_optimized:
            vocab_match = results_original['vocab'] == results_optimized['vocab']
            merges_match = results_original['merges'] == results_optimized['merges']
            
            print(f"\nğŸ” ç»“æœéªŒè¯:")
            print(f"   è¯æ±‡è¡¨ä¸€è‡´: {vocab_match}")
            print(f"   åˆå¹¶è§„åˆ™ä¸€è‡´: {merges_match}")
            
            if not vocab_match:
                print(f"   âš ï¸  è¯æ±‡è¡¨ä¸ä¸€è‡´ï¼")
            if not merges_match:
                print(f"   âš ï¸  åˆå¹¶è§„åˆ™ä¸ä¸€è‡´ï¼")
        
        # æ€§èƒ½è¯„çº§
        if speedup >= 2.0:
            print(f"\nğŸ† ä¼˜ç§€ï¼ä¼˜åŒ–æ•ˆæœæ˜¾è‘— (åŠ é€Ÿ {speedup:.1f}x)")
        elif speedup >= 1.5:
            print(f"\nğŸ‘ è‰¯å¥½ï¼ä¼˜åŒ–æ•ˆæœæ˜æ˜¾ (åŠ é€Ÿ {speedup:.1f}x)")
        elif speedup >= 1.2:
            print(f"\nâœ… ä¸€èˆ¬ï¼æœ‰ä¸€å®šä¼˜åŒ–æ•ˆæœ (åŠ é€Ÿ {speedup:.1f}x)")
        else:
            print(f"\nâš ï¸  ä¼˜åŒ–æ•ˆæœæœ‰é™ (åŠ é€Ÿ {speedup:.1f}x)")
    
    return original_time, optimized_time


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == "small":
            size_mb = 5
            vocab_size = 500
        elif sys.argv[1] == "medium":
            size_mb = 20
            vocab_size = 1000
        elif sys.argv[1] == "large":
            size_mb = 50
            vocab_size = 2000
        else:
            size_mb = int(sys.argv[1])
            vocab_size = int(sys.argv[2]) if len(sys.argv) > 2 else 1000
    else:
        size_mb = 10
        vocab_size = 1000
    
    print(f"åˆ›å»º {size_mb}MB æµ‹è¯•æ•°æ®...")
    test_file = create_test_data(size_mb)
    
    try:
        # è¿è¡Œæ€§èƒ½å¯¹æ¯”
        original_time, optimized_time = compare_performance(test_file, vocab_size)
        
        if original_time and optimized_time:
            print(f"\nğŸ¯ æ€»ç»“:")
            print(f"   åœ¨ {size_mb}MB æ•°æ®ä¸Šï¼Œä¼˜åŒ–ç‰ˆæœ¬æ¯”åŸå§‹ç‰ˆæœ¬å¿« {original_time/optimized_time:.2f} å€")
            print(f"   èŠ‚çœæ—¶é—´: {original_time - optimized_time:.2f} ç§’")
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            Path(test_file).unlink()
            print(f"\nğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {test_file}")
        except:
            pass


if __name__ == "__main__":
    print("BPEè®­ç»ƒæ€§èƒ½å¯¹æ¯”å·¥å…·")
    print("ä½¿ç”¨æ–¹æ³•:")
    print("  python compare_bpe_performance.py           # é»˜è®¤10MBæµ‹è¯•")
    print("  python compare_bpe_performance.py small     # 5MBæµ‹è¯•")
    print("  python compare_bpe_performance.py medium    # 20MBæµ‹è¯•")
    print("  python compare_bpe_performance.py large     # 50MBæµ‹è¯•")
    print("  python compare_bpe_performance.py <size_mb> [vocab_size]")
    print()
    
    main()
