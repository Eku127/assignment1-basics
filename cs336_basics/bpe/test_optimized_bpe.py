#!/usr/bin/env python3
"""
æµ‹è¯•ä¼˜åŒ–ç‰ˆBPEè®­ç»ƒå™¨
"""

import time
import tempfile
from pathlib import Path
from train_bpe_optimized import train_bpe_optimized


def test_small_dataset():
    """æµ‹è¯•å°æ•°æ®é›†"""
    print("=== æµ‹è¯•ä¼˜åŒ–ç‰ˆBPEè®­ç»ƒå™¨ ===\n")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_text = """Hello world! This is a test document.
<|endoftext|>
Another document with different content.
Machine learning is fascinating.
<|endoftext|>
Final document with numbers: 123 456 789.
Special characters: @#$%^&*()
<|endoftext|>"""
    
    # é‡å¤å¤šæ¬¡ä»¥å¢åŠ æ•°æ®é‡
    repeated_text = test_text * 100
    
    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:
        f.write(repeated_text)
        test_file = f.name
    
    print(f"æµ‹è¯•æ–‡ä»¶: {test_file}")
    print(f"æ–‡ä»¶å¤§å°: {len(repeated_text):,} å­—ç¬¦")
    
    try:
        # æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬
        print("\nğŸš€ æµ‹è¯•ä¼˜åŒ–ç‰ˆBPEè®­ç»ƒå™¨...")
        start_time = time.time()
        
        results = train_bpe_optimized(
            input_path=test_file,
            vocab_size=100,
            special_tokens=["<|endoftext|>"]
        )
        
        training_time = time.time() - start_time
        
        if results:
            print(f"âœ… è®­ç»ƒæˆåŠŸï¼")
            print(f"   è®­ç»ƒæ—¶é—´: {training_time:.3f} ç§’")
            print(f"   è¯æ±‡è¡¨å¤§å°: {len(results['vocab'])}")
            print(f"   åˆå¹¶è§„åˆ™æ•°: {len(results['merges'])}")
            
            # æ˜¾ç¤ºä¸€äº›å­¦ä¹ åˆ°çš„token
            learned_tokens = {k: v for k, v in results['vocab'].items() if k >= 256}
            print(f"\nğŸ“ å­¦ä¹ åˆ°çš„tokenç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
            for i, (token_id, token_bytes) in enumerate(sorted(learned_tokens.items())[:5]):
                try:
                    token_str = token_bytes.decode('utf-8', errors='replace')
                    print(f"     {token_id}: {repr(token_bytes)} -> '{token_str}'")
                except:
                    print(f"     {token_id}: {repr(token_bytes)}")
            
            return True
        else:
            print("âŒ è®­ç»ƒå¤±è´¥ï¼")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            Path(test_file).unlink()
            print(f"\nğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
        except:
            pass


def test_performance():
    """æµ‹è¯•æ€§èƒ½"""
    print("\n=== æ€§èƒ½æµ‹è¯• ===\n")
    
    # åˆ›å»ºæ›´å¤§çš„æµ‹è¯•æ•°æ®
    base_text = """This is a performance test document for BPE training.
It contains various words and patterns to test tokenization efficiency.
The quick brown fox jumps over the lazy dog multiple times.
Machine learning and natural language processing are complex fields.
<|endoftext|>
Another document with different vocabulary and sentence structures.
Deep learning models require extensive training data and computational resources.
Transformers have revolutionized the field of natural language processing.
Attention mechanisms allow models to focus on relevant parts of input sequences.
<|endoftext|>
Final document with numbers, symbols, and special characters: 123, 456, 789!
@#$%^&*() symbols and punctuation marks test robustness.
This comprehensive test helps evaluate the tokenizer's performance.
<|endoftext|>"""
    
    # åˆ›å»º10MBçš„æµ‹è¯•æ•°æ®
    target_size = 10 * 1024 * 1024  # 10MB
    repeated_text = base_text * (target_size // len(base_text) + 1)
    
    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:
        f.write(repeated_text)
        test_file = f.name
    
    print(f"æ€§èƒ½æµ‹è¯•æ–‡ä»¶: {test_file}")
    print(f"æ–‡ä»¶å¤§å°: {len(repeated_text) / (1024*1024):.2f} MB")
    
    try:
        print("\nğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•...")
        start_time = time.time()
        
        results = train_bpe_optimized(
            input_path=test_file,
            vocab_size=500,
            special_tokens=["<|endoftext|>"]
        )
        
        training_time = time.time() - start_time
        
        if results:
            print(f"âœ… æ€§èƒ½æµ‹è¯•æˆåŠŸï¼")
            print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
            print(f"   å¤„ç†é€Ÿåº¦: {len(repeated_text) / training_time / (1024*1024):.2f} MB/s")
            print(f"   è¯æ±‡è¡¨å¤§å°: {len(results['vocab'])}")
            print(f"   åˆå¹¶è§„åˆ™æ•°: {len(results['merges'])}")
            
            # æ€§èƒ½è¯„çº§
            if training_time < 30:
                print("ğŸ† ä¼˜ç§€ï¼è®­ç»ƒé€Ÿåº¦å¾ˆå¿«")
            elif training_time < 60:
                print("ğŸ‘ è‰¯å¥½ï¼è®­ç»ƒé€Ÿåº¦ä¸é”™")
            else:
                print("âš ï¸  è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            
            return True
        else:
            print("âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥ï¼")
            return False
            
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            Path(test_file).unlink()
            print(f"\nğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
        except:
            pass


if __name__ == "__main__":
    print("ä¼˜åŒ–ç‰ˆBPEè®­ç»ƒå™¨æµ‹è¯•")
    print("=" * 50)
    
    # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    success1 = test_small_dataset()
    
    if success1:
        # æ€§èƒ½æµ‹è¯•
        success2 = test_performance()
        
        if success2:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¼˜åŒ–ç‰ˆBPEè®­ç»ƒå™¨å·¥ä½œæ­£å¸¸ã€‚")
        else:
            print("\nâš ï¸  åŸºæœ¬åŠŸèƒ½æ­£å¸¸ï¼Œä½†æ€§èƒ½æµ‹è¯•å¤±è´¥ã€‚")
    else:
        print("\nâŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ã€‚")
