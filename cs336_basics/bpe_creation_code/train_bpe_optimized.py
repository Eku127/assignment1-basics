#!/usr/bin/env python3
"""
è®­ç»ƒBPEåˆ†è¯å™¨ - ä¼˜åŒ–ç‰ˆæœ¬
åŒ…å«å¤šç§æ€§èƒ½ä¼˜åŒ–ï¼šå¢é‡æ›´æ–°ã€å †ä¼˜åŒ–ã€å¹¶è¡Œå¤„ç†ç­‰
"""

import time
import json
from pathlib import Path
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
from collections import defaultdict, Counter
import regex as regex_mod
import re
import heapq
from functools import lru_cache
from cs336_basics.bpe import train_bpe


def _pretokenize_chunk_batch(chunks: list[str]) -> dict[tuple[int, ...], int]:
    """æ‰¹é‡é¢„åˆ†è¯å¤šä¸ªæ–‡æ¡£å—ï¼Œå‡å°‘è¿›ç¨‹é—´é€šä¿¡"""
    PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    pat = regex_mod.compile(PAT)
    freq: dict[tuple[int, ...], int] = defaultdict(int)
    
    for chunk in chunks:
        for m in pat.finditer(chunk):
            token = m.group(0)
            bs = token.encode("utf-8")
            freq[tuple(bs)] += 1
    
    return dict(freq)


def _pretokenize_parallel_optimized(segments: list[str], num_processes: int = None) -> dict[tuple[int, ...], int]:
    """ä¼˜åŒ–çš„å¤šè¿›ç¨‹å¹¶è¡Œé¢„åˆ†è¯"""
    if num_processes is None:
        num_processes = min(cpu_count(), len(segments))
    
    # å°†æ–‡æ¡£åˆ†æ‰¹ï¼Œå‡å°‘è¿›ç¨‹é—´é€šä¿¡æ¬¡æ•°
    batch_size = max(1, len(segments) // (num_processes * 4))
    batches = [segments[i:i + batch_size] for i in range(0, len(segments), batch_size)]
    
    print(f"   ä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹å¤„ç† {len(batches)} ä¸ªæ‰¹æ¬¡...")
    
    with Pool(processes=num_processes) as pool:
        batch_freqs = list(tqdm(
            pool.imap(_pretokenize_chunk_batch, batches),
            total=len(batches),
            desc="é¢„åˆ†è¯è¿›åº¦",
            unit="æ‰¹æ¬¡"
        ))
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    total_freq: dict[tuple[int, ...], int] = defaultdict(int)
    for batch_freq in batch_freqs:
        for token_tuple, count in batch_freq.items():
            total_freq[token_tuple] += count
    
    return dict(total_freq)


def _pretokenize_single_thread_optimized(segments: list[str]) -> dict[tuple[int, ...], int]:
    """å•çº¿ç¨‹ä¼˜åŒ–ç‰ˆæœ¬"""
    PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    pat = regex_mod.compile(PAT)
    freq: dict[tuple[int, ...], int] = defaultdict(int)
    
    print(f"   ä½¿ç”¨å•çº¿ç¨‹ä¼˜åŒ–å¤„ç†...")
    
    for chunk in tqdm(segments, desc="é¢„åˆ†è¯è¿›åº¦", unit="æ®µè½"):
        tokens = pat.findall(chunk)
        for token in tokens:
            bs = token.encode("utf-8")
            freq[tuple(bs)] += 1
    
    return dict(freq)


class OptimizedBPETrainer:
    """ä¼˜åŒ–çš„BPEè®­ç»ƒå™¨ç±»"""
    
    def __init__(self, vocab_size: int, special_tokens: list[str]):
        self.vocab_size = vocab_size
        self.special_tokens = special_tokens
        self.id_to_bytes = {}
        self.bytes_to_id = {}
        self.next_id = 0
        
        # ä¼˜åŒ–çš„æ•°æ®ç»“æ„
        self.corpus_ids = []
        self.corpus_counts = []
        self.word_pair_counters = []
        self.total_pair_counts = defaultdict(int)
        self.pair_to_words = defaultdict(set)
        
        # å †ä¼˜åŒ–ï¼šç»´æŠ¤æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹
        self.pair_heap = []
        self.heap_rebuild_needed = True
        
        # ç¼“å­˜ä¼˜åŒ–ï¼ˆä½¿ç”¨@lru_cacheè£…é¥°å™¨ï¼‰
        
    def _build_initial_vocab(self):
        """åˆå§‹åŒ–è¯æ±‡è¡¨"""
        # 256å­—èŠ‚è¯æ±‡è¡¨
        for b in range(256):
            bb = bytes([b])
            self.id_to_bytes[b] = bb
            self.bytes_to_id[bb] = b
        
        self.next_id = 256
        # ç‰¹æ®Štoken
        for tok in self.special_tokens:
            b = tok.encode("utf-8")
            self.id_to_bytes[self.next_id] = b
            self.bytes_to_id[b] = self.next_id
            self.next_id += 1
    
    @lru_cache(maxsize=10000)
    def _get_pair_key(self, pair: tuple[int, int]) -> tuple[int, bytes, bytes]:
        """ç¼“å­˜çš„å­—èŠ‚å¯¹é”®è®¡ç®—"""
        a, b = pair
        return (self.total_pair_counts[pair], self.id_to_bytes[a], self.id_to_bytes[b])
    
    def _rebuild_heap(self):
        """é‡å»ºå­—èŠ‚å¯¹å †"""
        self.pair_heap = []
        # åªå¤„ç†é¢‘ç‡å¤§äº0çš„å­—èŠ‚å¯¹
        for pair, freq in self.total_pair_counts.items():
            if freq > 0:
                # ç›´æ¥ä½¿ç”¨é¢‘ç‡ï¼Œé¿å…é‡å¤è®¡ç®—
                bytes_a = self.id_to_bytes[pair[0]]
                bytes_b = self.id_to_bytes[pair[1]]
                heapq.heappush(self.pair_heap, (-freq, bytes_a, bytes_b, pair))
        self.heap_rebuild_needed = False
    
    def _get_best_pair(self) -> tuple[int, int] | None:
        """è·å–æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹ï¼ˆä½¿ç”¨å †ä¼˜åŒ–ï¼‰"""
        if not self.total_pair_counts:
            return None
        
        if self.heap_rebuild_needed:
            self._rebuild_heap()
        
        # ä»å †ä¸­è·å–æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹
        while self.pair_heap:
            neg_freq, bytes_a, bytes_b, pair = heapq.heappop(self.pair_heap)
            # æ£€æŸ¥é¢‘ç‡æ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼ˆå¯èƒ½åœ¨å…¶ä»–åœ°æ–¹è¢«æ›´æ–°ï¼‰
            current_freq = self.total_pair_counts.get(pair, 0)
            if current_freq > 0:
                return pair
        
        return None
    
    def _pairs_for_ids(self, ids: list[int]) -> Counter[tuple[int, int]]:
        """è®¡ç®—IDåºåˆ—çš„å­—èŠ‚å¯¹"""
        ctr = Counter()
        if len(ids) < 2:
            return ctr
        for a, b in zip(ids, ids[1:]):
            ctr[(a, b)] += 1
        return ctr
    
    def _merge_pair_optimized(self, a: int, b: int, new_token: int) -> None:
        """ä¼˜åŒ–çš„å­—èŠ‚å¯¹åˆå¹¶å‡½æ•°"""
        target = (a, b)
        affected_words = list(self.pair_to_words.get(target, set()))
        
        if len(affected_words) < 100:  # å°æ‰¹é‡ç”¨ä¸²è¡Œå¤„ç†
            self._merge_pair_serial(a, b, new_token, affected_words)
        else:  # å¤§æ‰¹é‡ç”¨å¹¶è¡Œå¤„ç†
            self._merge_pair_parallel(a, b, new_token, affected_words)
        
        # æ ‡è®°éœ€è¦é‡å»ºå †
        self.heap_rebuild_needed = True
    
    def _merge_pair_serial(self, a: int, b: int, new_token: int, affected_words: list) -> None:
        """ä¸²è¡Œåˆå¹¶å¤„ç†"""
        batch_updates = []
        
        for i in affected_words:
            ids = self.corpus_ids[i]
            if len(ids) < 2:
                continue
            
            old_pairs = self.word_pair_counters[i]
            count_multiplier = self.corpus_counts[i]
            
            # æ”¶é›†éœ€è¦ç§»é™¤çš„å­—èŠ‚å¯¹
            for pair, k in old_pairs.items():
                self.total_pair_counts[pair] -= k * count_multiplier
                if self.total_pair_counts[pair] <= 0:
                    self.total_pair_counts.pop(pair, None)
            
            # æ‰§è¡Œåˆå¹¶
            out = self._perform_merge(ids, a, b, new_token)
            self.corpus_ids[i] = out
            
            # é‡æ–°è®¡ç®—å­—èŠ‚å¯¹
            new_ctr = self._pairs_for_ids(out)
            self.word_pair_counters[i] = new_ctr
            
            # æ”¶é›†æ‰¹é‡æ›´æ–°
            batch_updates.append((i, old_pairs, new_ctr, count_multiplier))
        
        # æ‰¹é‡åº”ç”¨æ›´æ–°
        self._apply_batch_updates(batch_updates)
    
    def _merge_pair_parallel(self, a: int, b: int, new_token: int, affected_words: list) -> None:
        """å¹¶è¡Œåˆå¹¶å¤„ç†"""
        # å°†å—å½±å“çš„è¯åˆ†æ‰¹
        batch_size = max(1, len(affected_words) // (cpu_count() * 2))
        word_batches = [affected_words[i:i + batch_size] for i in range(0, len(affected_words), batch_size)]
        
        # å‡†å¤‡å¹¶è¡Œå¤„ç†çš„æ•°æ®
        merge_tasks = []
        for batch in word_batches:
            task_data = []
            for i in batch:
                if i < len(self.corpus_ids) and len(self.corpus_ids[i]) >= 2:
                    task_data.append({
                        'word_id': i,
                        'ids': self.corpus_ids[i].copy(),
                        'old_pairs': dict(self.word_pair_counters[i]),
                        'count_multiplier': self.corpus_counts[i]
                    })
            if task_data:
                merge_tasks.append((a, b, new_token, task_data))
        
        # å¹¶è¡Œå¤„ç†
        with Pool(processes=min(cpu_count(), len(merge_tasks))) as pool:
            results = pool.map(self._process_merge_batch, merge_tasks)
        
        # æ”¶é›†ç»“æœå¹¶åº”ç”¨æ›´æ–°
        all_batch_updates = []
        for batch_results in results:
            all_batch_updates.extend(batch_results)
        
        # æ‰¹é‡åº”ç”¨æ‰€æœ‰æ›´æ–°
        self._apply_batch_updates(all_batch_updates)
    
    @staticmethod
    def _process_merge_batch(args):
        """å¤„ç†å•ä¸ªåˆå¹¶æ‰¹æ¬¡ï¼ˆé™æ€æ–¹æ³•ï¼Œç”¨äºå¤šè¿›ç¨‹ï¼‰"""
        a, b, new_token, task_data = args
        batch_updates = []
        
        for task in task_data:
            word_id = task['word_id']
            ids = task['ids']
            old_pairs = task['old_pairs']
            count_multiplier = task['count_multiplier']
            
            # æ‰§è¡Œåˆå¹¶
            out = OptimizedBPETrainer._perform_merge_static(ids, a, b, new_token)
            
            # é‡æ–°è®¡ç®—å­—èŠ‚å¯¹
            new_ctr = OptimizedBPETrainer._pairs_for_ids_static(out)
            
            # æ”¶é›†æ›´æ–°
            batch_updates.append((word_id, old_pairs, new_ctr, count_multiplier))
        
        return batch_updates
    
    def _perform_merge(self, ids: list[int], a: int, b: int, new_token: int) -> list[int]:
        """æ‰§è¡Œå­—èŠ‚å¯¹åˆå¹¶"""
        j = 0
        out = []
        while j < len(ids):
            if j < len(ids) - 1 and ids[j] == a and ids[j + 1] == b:
                out.append(new_token)
                j += 2
            else:
                out.append(ids[j])
                j += 1
        return out
    
    @staticmethod
    def _perform_merge_static(ids: list[int], a: int, b: int, new_token: int) -> list[int]:
        """é™æ€æ–¹æ³•ç‰ˆæœ¬çš„åˆå¹¶ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰"""
        j = 0
        out = []
        while j < len(ids):
            if j < len(ids) - 1 and ids[j] == a and ids[j + 1] == b:
                out.append(new_token)
                j += 2
            else:
                out.append(ids[j])
                j += 1
        return out
    
    @staticmethod
    def _pairs_for_ids_static(ids: list[int]) -> dict:
        """é™æ€æ–¹æ³•ç‰ˆæœ¬çš„å­—èŠ‚å¯¹è®¡ç®—ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰"""
        ctr = {}
        if len(ids) < 2:
            return ctr
        for a, b in zip(ids, ids[1:]):
            pair = (a, b)
            ctr[pair] = ctr.get(pair, 0) + 1
        return ctr
    
    def _apply_batch_updates(self, batch_updates: list):
        """æ‰¹é‡åº”ç”¨ç´¢å¼•æ›´æ–°ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        # ä½¿ç”¨é›†åˆæ¥è·Ÿè¸ªéœ€è¦æ¸…ç†å’Œæ·»åŠ çš„å­—èŠ‚å¯¹
        pairs_to_remove = set()
        pairs_to_add = {}
        
        # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰éœ€è¦æ¸…ç†çš„å­—èŠ‚å¯¹
        for i, old_pairs, new_ctr, count_multiplier in batch_updates:
            for pair in old_pairs.keys():
                if pair not in new_ctr:
                    pairs_to_remove.add(pair)
        
        # ç¬¬äºŒéï¼šæ”¶é›†æ‰€æœ‰éœ€è¦æ·»åŠ çš„å­—èŠ‚å¯¹
        for i, old_pairs, new_ctr, count_multiplier in batch_updates:
            for pair, k in new_ctr.items():
                if pair not in pairs_to_add:
                    pairs_to_add[pair] = []
                pairs_to_add[pair].append((i, k * count_multiplier))
        
        # æ‰¹é‡æ¸…ç†æ—§ç´¢å¼•
        for pair in pairs_to_remove:
            s = self.pair_to_words.get(pair)
            if s is not None:
                # æ‰¹é‡ç§»é™¤æ‰€æœ‰ç›¸å…³çš„è¯
                for i, old_pairs, new_ctr, count_multiplier in batch_updates:
                    s.discard(i)
                if not s:
                    self.pair_to_words.pop(pair, None)
        
        # æ‰¹é‡æ·»åŠ æ–°ç´¢å¼•
        for pair, updates in pairs_to_add.items():
            # è®¡ç®—æ€»é¢‘ç‡
            total_freq = sum(freq for _, freq in updates)
            self.total_pair_counts[pair] = self.total_pair_counts.get(pair, 0) + total_freq
            
            # æ·»åŠ è¯ç´¢å¼•
            s = self.pair_to_words.get(pair)
            if s is None:
                s = set()
                self.pair_to_words[pair] = s
            
            for i, _ in updates:
                s.add(i)
    
    def train(self, input_path: str) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
        """è®­ç»ƒBPEåˆ†è¯å™¨"""
        print("=== ä¼˜åŒ–ç‰ˆBPEè®­ç»ƒ ===\n")
        
        # 1. è¯»å–æ–‡ä»¶
        print("ğŸ“– è¯»å–è®­ç»ƒæ•°æ®...")
        with open(input_path, "r", encoding="utf-8") as f:
            text = f.read()
        
        file_size = len(text)
        print(f"   æ–‡ä»¶å¤§å°: {file_size:,} å­—ç¬¦")
        
        # 2. åˆ†å‰²ç‰¹æ®Štoken
        print("ğŸ”§ å¤„ç†ç‰¹æ®Štoken...")
        if not self.special_tokens:
            segments = [text]
        else:
            escaped = [re.escape(tok) for tok in self.special_tokens]
            pattern = re.compile("(" + "|".join(escaped) + ")")
            parts = pattern.split(text)
            segments = [seg for seg in parts if seg and seg not in self.special_tokens]
        
        print(f"   åˆ†å‰²æˆ {len(segments)} ä¸ªæ®µè½")
        
        # 3. é¢„åˆ†è¯
        print("âœ‚ï¸  é¢„åˆ†è¯...")
        if len(segments) < 10000:
            freq = _pretokenize_single_thread_optimized(segments)
        else:
            freq = _pretokenize_parallel_optimized(segments)
        
        print(f"   é¢„åˆ†è¯å®Œæˆï¼Œå…± {len(freq):,} ä¸ªå”¯ä¸€token")
        
        # 4. åˆå§‹åŒ–è¯æ±‡è¡¨
        print("ğŸ“š åˆå§‹åŒ–è¯æ±‡è¡¨...")
        self._build_initial_vocab()
        print(f"   åˆå§‹è¯æ±‡è¡¨å¤§å°: {len(self.id_to_bytes)}")
        
        # 5. å‡†å¤‡è¯­æ–™åº“
        print("ğŸ“ å‡†å¤‡è¯­æ–™åº“...")
        for byte_tuple, count in tqdm(freq.items(), desc="è½¬æ¢è¯­æ–™åº“", unit="token"):
            ids = [self.bytes_to_id[bytes([b])] for b in byte_tuple]
            self.corpus_ids.append(ids)
            self.corpus_counts.append(count)
        
        print(f"   è¯­æ–™åº“åŒ…å« {len(self.corpus_ids):,} ä¸ªtokenåºåˆ—")
        
        # 6. åˆå§‹åŒ–å­—èŠ‚å¯¹è®¡æ•°
        print("ğŸ”¢ åˆå§‹åŒ–å­—èŠ‚å¯¹è®¡æ•°...")
        for i, ids in enumerate(tqdm(self.corpus_ids, desc="è®¡ç®—å­—èŠ‚å¯¹", unit="åºåˆ—")):
            ctr = self._pairs_for_ids(ids)
            self.word_pair_counters.append(ctr)
            c = self.corpus_counts[i]
            for pair, k in ctr.items():
                self.total_pair_counts[pair] += k * c
                self.pair_to_words[pair].add(i)
        
        print(f"   å‘ç° {len(self.total_pair_counts):,} ä¸ªå”¯ä¸€å­—èŠ‚å¯¹")
        
        # 7. ä¸»åˆå¹¶å¾ªç¯
        merges = []
        max_merges = max(0, self.vocab_size - len(self.id_to_bytes))
        
        print(f"ğŸ”„ å¼€å§‹åˆå¹¶å¾ªç¯ï¼Œç›®æ ‡: {max_merges:,} æ¬¡åˆå¹¶")
        
        # æ€§èƒ½ç»Ÿè®¡
        merge_times = []
        parallel_count = 0
        serial_count = 0
        
        with tqdm(total=max_merges, desc="åˆå¹¶è¿›åº¦", unit="åˆå¹¶") as pbar:
            for merge_step in range(max_merges):
                merge_start = time.time()
                
                best_pair = self._get_best_pair()
                if best_pair is None:
                    pbar.set_description("åˆå¹¶å®Œæˆ (æ— æ›´å¤šå­—èŠ‚å¯¹)")
                    break
                
                a, b = best_pair
                
                # åˆ›å»ºæ–°token
                bytes_a = self.id_to_bytes[a]
                bytes_b = self.id_to_bytes[b]
                new_bytes = bytes_a + bytes_b
                new_id = self.next_id
                self.next_id += 1
                self.id_to_bytes[new_id] = new_bytes
                
                # æ‰§è¡Œåˆå¹¶
                affected_count = len(self.pair_to_words.get((a, b), set()))
                if affected_count >= 100:
                    parallel_count += 1
                else:
                    serial_count += 1
                
                self._merge_pair_optimized(a, b, new_id)
                merges.append((bytes_a, bytes_b))
                
                merge_time = time.time() - merge_start
                merge_times.append(merge_time)
                
                # æ›´æ–°è¿›åº¦æ¡
                avg_time = sum(merge_times[-10:]) / min(10, len(merge_times))
                pbar.set_postfix({
                    'å½“å‰åˆå¹¶': f"{bytes_a} + {bytes_b}",
                    'è¯æ±‡è¡¨å¤§å°': len(self.id_to_bytes),
                    'å‰©ä½™å­—èŠ‚å¯¹': len(self.total_pair_counts),
                    'å½±å“è¯æ•°': affected_count,
                    'å¹³å‡æ—¶é—´': f"{avg_time:.3f}s"
                })
                pbar.update(1)
                
                if len(self.id_to_bytes) >= self.vocab_size:
                    pbar.set_description("åˆå¹¶å®Œæˆ (è¾¾åˆ°ç›®æ ‡è¯æ±‡è¡¨å¤§å°)")
                    break
        
        # è¾“å‡ºæ€§èƒ½ç»Ÿè®¡
        if merge_times:
            print(f"\nğŸ“Š åˆå¹¶æ€§èƒ½ç»Ÿè®¡:")
            print(f"   æ€»åˆå¹¶æ¬¡æ•°: {len(merge_times)}")
            print(f"   å¹¶è¡Œå¤„ç†æ¬¡æ•°: {parallel_count}")
            print(f"   ä¸²è¡Œå¤„ç†æ¬¡æ•°: {serial_count}")
            print(f"   å¹³å‡åˆå¹¶æ—¶é—´: {sum(merge_times) / len(merge_times):.3f} ç§’")
            print(f"   æœ€å¿«åˆå¹¶æ—¶é—´: {min(merge_times):.3f} ç§’")
            print(f"   æœ€æ…¢åˆå¹¶æ—¶é—´: {max(merge_times):.3f} ç§’")
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
        print(f"   æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(self.id_to_bytes):,}")
        print(f"   åˆå¹¶è§„åˆ™æ•°é‡: {len(merges):,}")
        
        return self.id_to_bytes, merges


def detect_dataset(input_path: str) -> str:
    """æ£€æµ‹æ•°æ®é›†ç±»å‹"""
    path = Path(input_path)
    if "tinystories" in path.name.lower():
        return "tinystories"
    elif "owt" in path.name.lower() or "openwebtext" in path.name.lower():
        return "owt"
    else:
        return "unknown"


def get_dataset_config(dataset_type: str) -> dict:
    """è·å–æ•°æ®é›†é…ç½®"""
    configs = {
        "tinystories": {
            "vocab_size": 10000,
            "special_tokens": ["<|endoftext|>"],
            "description": "TinyStoriesæ•°æ®é›†"
        },
        "owt": {
            "vocab_size": 32000,
            "special_tokens": ["<|endoftext|>"],
            "description": "OpenWebTextæ•°æ®é›†"
        },
        "unknown": {
            "vocab_size": 10000,
            "special_tokens": ["<|endoftext|>"],
            "description": "æœªçŸ¥æ•°æ®é›†"
        }
    }
    return configs.get(dataset_type, configs["unknown"])


def train_bpe_optimized(input_path: str, dataset_type: str = None, vocab_size: int = None, special_tokens: list[str] = None):
    """
    ä¼˜åŒ–çš„BPEè®­ç»ƒå‡½æ•°
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        dataset_type: æ•°æ®é›†ç±»å‹ ("tinystories", "owt", "auto")
        vocab_size: è¯æ±‡è¡¨å¤§å°ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
        special_tokens: ç‰¹æ®Štokenåˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
    """
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç±»å‹
    if dataset_type is None or dataset_type == "auto":
        dataset_type = detect_dataset(input_path)
    
    # è·å–æ•°æ®é›†é…ç½®
    config = get_dataset_config(dataset_type)
    
    # ä½¿ç”¨ç”¨æˆ·æä¾›çš„å‚æ•°æˆ–é»˜è®¤é…ç½®
    if vocab_size is None:
        vocab_size = config["vocab_size"]
    if special_tokens is None:
        special_tokens = config["special_tokens"]
    
    print(f"=== ä¼˜åŒ–ç‰ˆBPEè®­ç»ƒå™¨åœ¨{config['description']}ä¸Š ===\n")
    print(f"æ£€æµ‹åˆ°çš„æ•°æ®é›†ç±»å‹: {dataset_type}")
    print(f"é…ç½®å‚æ•°:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  ç‰¹æ®Štoken: {special_tokens}")
    print(f"  è¾“å…¥æ–‡ä»¶: {input_path}")
    print()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(input_path).exists():
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {input_path}")
        return None
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    try:
        # åˆ›å»ºä¼˜åŒ–è®­ç»ƒå™¨
        trainer = OptimizedBPETrainer(vocab_size, special_tokens)
        
        # è®­ç»ƒBPEåˆ†è¯å™¨
        vocab, merges = trainer.train(input_path)
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        training_time = end_time - start_time
        
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’ ({training_time/60:.2f} åˆ†é’Ÿ)")
        print(f"   æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        print(f"   åˆå¹¶è§„åˆ™æ•°é‡: {len(merges)}")
        
        # åˆ†æè¯æ±‡è¡¨
        print(f"\nğŸ” è¯æ±‡è¡¨åˆ†æ:")
        
        # æ‰¾å‡ºæœ€é•¿çš„token
        longest_token = max(vocab.values(), key=len)
        print(f"   æœ€é•¿token: {repr(longest_token)} (é•¿åº¦: {len(longest_token)})")
        
        try:
            longest_str = longest_token.decode('utf-8', errors='replace')
            print(f"   æœ€é•¿tokenè§£ç : '{longest_str}'")
        except:
            print(f"   æœ€é•¿tokenæ— æ³•è§£ç ä¸ºUTF-8")
        
        # ç»Ÿè®¡ä¸åŒé•¿åº¦çš„token
        length_counts = {}
        for token_bytes in vocab.values():
            length = len(token_bytes)
            length_counts[length] = length_counts.get(length, 0) + 1
        
        print(f"\n   Tokené•¿åº¦åˆ†å¸ƒ:")
        for length in sorted(length_counts.keys()):
            print(f"     é•¿åº¦ {length}: {length_counts[length]:,} ä¸ªtoken")
        
        # æ˜¾ç¤ºä¸€äº›å­¦ä¹ åˆ°çš„token
        learned_tokens = {k: v for k, v in vocab.items() if k >= 256}
        print(f"\n   ğŸ“ å­¦ä¹ åˆ°çš„tokenç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰:")
        for i, (token_id, token_bytes) in enumerate(sorted(learned_tokens.items())[:10]):
            try:
                token_str = token_bytes.decode('utf-8', errors='replace')
                print(f"     {token_id}: {repr(token_bytes)} -> '{token_str}'")
            except:
                print(f"     {token_id}: {repr(token_bytes)}")
        
        # åºåˆ—åŒ–ç»“æœåˆ°ç£ç›˜
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœ...")
        output_dir = Path("bpe_results")
        output_dir.mkdir(exist_ok=True)
        
        # æ ¹æ®æ•°æ®é›†ç±»å‹ç”Ÿæˆæ–‡ä»¶å
        dataset_prefix = dataset_type if dataset_type != "unknown" else "custom"
        
        # ä¿å­˜è¯æ±‡è¡¨ï¼ˆJSONæ ¼å¼ï¼‰
        vocab_json = {str(k): list(v) for k, v in vocab.items()}
        vocab_file = output_dir / f"{dataset_prefix}_vocab_optimized.json"
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump(vocab_json, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜åˆå¹¶è§„åˆ™ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
        merges_file = output_dir / f"{dataset_prefix}_merges_optimized.txt"
        with open(merges_file, 'w', encoding='utf-8') as f:
            for left, right in merges:
                f.write(f"{left} {right}\n")
        
        print(f"   ç»“æœå·²ä¿å­˜åˆ°:")
        print(f"     ğŸ“„ è¯æ±‡è¡¨: {vocab_file}")
        print(f"     ğŸ“„ åˆå¹¶è§„åˆ™: {merges_file}")
        
        # éªŒè¯ç»“æœ
        print(f"\nâœ… éªŒè¯ç»“æœ:")
        special_token_bytes = '<|endoftext|>'.encode('utf-8')
        has_special_token = any(token == special_token_bytes for token in vocab.values())
        print(f"   ç‰¹æ®Štoken <|endoftext|> åœ¨è¯æ±‡è¡¨ä¸­: {has_special_token}")
        print(f"   è¯æ±‡è¡¨å¤§å°ç¬¦åˆè¦æ±‚: {len(vocab) == vocab_size}")
        print(f"   åˆå¹¶è§„åˆ™æ•°é‡: {len(merges)}")
        
        return {
            'vocab': vocab,
            'merges': merges,
            'training_time': training_time,
            'vocab_size': vocab_size,
            'special_tokens': special_tokens,
            'dataset_type': dataset_type
        }
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


def train_bpe_tinystories_optimized():
    """åœ¨TinyStoriesæ•°æ®é›†ä¸Šè®­ç»ƒBPEåˆ†è¯å™¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    input_path = "./data/TinyStoriesV2-GPT4-train.txt"
    return train_bpe_optimized(input_path, dataset_type="tinystories")


def train_bpe_owt_optimized():
    """åœ¨OpenWebTextæ•°æ®é›†ä¸Šè®­ç»ƒBPEåˆ†è¯å™¨ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    input_path = "./data/owt_train.txt"
    return train_bpe_optimized(input_path, dataset_type="owt")


if __name__ == "__main__":
    import sys
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        dataset_type = sys.argv[1].lower()
        if dataset_type == "tinystories":
            results = train_bpe_tinystories_optimized()
        elif dataset_type == "owt":
            results = train_bpe_owt_optimized()
        elif dataset_type == "auto":
            # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†
            input_path = sys.argv[2] if len(sys.argv) > 2 else "./data/TinyStoriesV2-GPT4-train.txt"
            results = train_bpe_optimized(input_path, dataset_type="auto")
        else:
            # è‡ªå®šä¹‰æ•°æ®é›†
            input_path = sys.argv[1]
            vocab_size = int(sys.argv[2]) if len(sys.argv) > 2 else None
            results = train_bpe_optimized(input_path, vocab_size=vocab_size)
    else:
        # é»˜è®¤è®­ç»ƒTinyStories
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python train_bpe_optimized.py tinystories")
        print("  python train_bpe_optimized.py owt")
        print("  python train_bpe_optimized.py auto <input_path>")
        print("  python train_bpe_optimized.py <input_path> [vocab_size]")
        print()
        print("é»˜è®¤è®­ç»ƒTinyStoriesæ•°æ®é›†...")
        results = train_bpe_tinystories_optimized()
    
    if results:
        print("\nğŸ‰ ä¼˜åŒ–ç‰ˆè®­ç»ƒå®Œæˆï¼")
    else:
        print("\nğŸ’¥ ä¼˜åŒ–ç‰ˆè®­ç»ƒå¤±è´¥ï¼")
