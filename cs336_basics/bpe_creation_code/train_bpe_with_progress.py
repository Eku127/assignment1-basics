#!/usr/bin/env python3
"""
è®­ç»ƒBPEåˆ†è¯å™¨åœ¨TinyStoriesæ•°æ®é›†ä¸Š - å¸¦è¿›åº¦æ¡ç‰ˆæœ¬

è®­ç»ƒæ„Ÿæƒ³ï¼š
åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ä¼šå‘ç° total_pair_counts ä¼šä¸æ–­å¢åŠ ï¼Œè¿™ä¸ªç°è±¡æ˜¯åˆç†çš„ï¼
å› ä¸ºåœ¨è®­ç»ƒçš„åˆæœŸ, åˆå¹¶çš„åªæœ‰å¾ˆå°‘çš„pairs,ä½†æ˜¯æ–°äº§ç”Ÿçš„tokenä¼šäº§ç”Ÿå¤§é‡æ–°çš„token!
æ¯”å¦‚è¿™ä¸ªä¾‹å­:
[DEBUG] --- MERGE STEP 1 ---
[DEBUG] Merging pair: (b' ', b't') -> ID 257
[DEBUG] Initial len(total_pair_counts): 932
    (-) Pairs REMOVED (2 unique types): { (b' ', b't'), (b't', b'v') }
    (+) Pairs ADDED   (10 unique types): { (b' t', b'a'), (b' t', b'e'), (b' t', b'h'), (b' t', b'i'), (b' t', b'o'), (b' t', b'r'), (b' t', b'u'), (b' t', b'v'), (b' t', b'w'), (b' t', b'y') }
[DEBUG] Final len(total_pair_counts): 940
[DEBUG] Net change in len for this step: 8  (10 added - 2 removed)

å¯ä»¥å‘ç°, åˆå¹¶äº† (b' ', b't') å’Œ (b't', b'v') ä¹‹å, æ–°äº§ç”Ÿäº† 10 ä¸ªæ–°çš„token! 
ä¹‹å‰(b't', b'h')å¦‚æœå­˜åœ¨çš„è¯ï¼Œé‚£ä¹ˆæœ‰äº†æ–°çš„token b' t'ä¹‹åï¼Œå°±ä¼šè¿˜ä¼šäº§ç”Ÿå¦å¤–ä¸€ä¸ªæ–°çš„pair(b' t', b'h')!
è¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆä¼šå‡ºç°ä¸€ç›´ä¸Šæ¶¨çš„æƒ…å†µ

è¿™ä¸ªç°è±¡å®Œå…¨å–å†³äºè¯­æ–™åº“çš„å¤§å°ä»¥åŠvocab_sizeçš„è®¾å®šï¼Œå¦‚æœvocab sizeè®¾ç½®è¶³å¤Ÿå¤§çš„è¯ï¼Œä¸€å®šèƒ½çœ‹åˆ°pairsçš„ä¸Šå‡å’Œä¸‹é™

å¯ä»¥ä½¿ç”¨merge_pair_debugå‡½æ•°æ¥çœ‹åˆ°è¿™ä¸ªè¿‡ç¨‹

"""

import time
import json
from pathlib import Path
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
from collections import defaultdict, Counter
import regex as regex_mod
import re

merge_step_counter = 0


def _pretokenize_chunk_batch(chunks: list[str]) -> dict[tuple[int, ...], int]:
    """æ‰¹é‡é¢„åˆ†è¯å¤šä¸ªæ–‡æ¡£å—ï¼Œå‡å°‘è¿›ç¨‹é—´é€šä¿¡"""
    PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    pat = regex_mod.compile(PAT)
    freq: dict[tuple[int, ...], int] = defaultdict(int)
    
    for chunk in chunks:
        for m in pat.finditer(chunk):
            token = m.group(0)
            bs = token.encode("utf-8")
            freq[tuple(bs)] += 1
    
    return dict(freq)


def _pretokenize_parallel_optimized(segments: list[str], num_processes: int = None) -> dict[tuple[int, ...], int]:
    """ä¼˜åŒ–çš„å¤šè¿›ç¨‹å¹¶è¡Œé¢„åˆ†è¯"""
    if num_processes is None:
        num_processes = min(cpu_count(), len(segments))
    
    # å°†æ–‡æ¡£åˆ†æ‰¹ï¼Œå‡å°‘è¿›ç¨‹é—´é€šä¿¡æ¬¡æ•°
    batch_size = max(1, len(segments) // (num_processes * 4))  # æ¯ä¸ªè¿›ç¨‹å¤„ç†å¤šä¸ªæ–‡æ¡£
    batches = [segments[i:i + batch_size] for i in range(0, len(segments), batch_size)]
    
    print(f"   ä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹å¤„ç† {len(batches)} ä¸ªæ‰¹æ¬¡...")
    
    # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
    with Pool(processes=num_processes) as pool:
        batch_freqs = list(tqdm(
            pool.imap(_pretokenize_chunk_batch, batches),
            total=len(batches),
            desc="é¢„åˆ†è¯è¿›åº¦",
            unit="æ‰¹æ¬¡"
        ))
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    total_freq: dict[tuple[int, ...], int] = defaultdict(int)
    for batch_freq in batch_freqs:
        for token_tuple, count in batch_freq.items():
            total_freq[token_tuple] += count
    
    return dict(total_freq)


def _pretokenize_single_thread_optimized(segments: list[str]) -> dict[tuple[int, ...], int]:
    """å•çº¿ç¨‹ä¼˜åŒ–ç‰ˆæœ¬ - é¿å…è¿›ç¨‹é—´é€šä¿¡å¼€é”€"""
    PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    pat = regex_mod.compile(PAT)
    freq: dict[tuple[int, ...], int] = defaultdict(int)
    
    print(f"   ä½¿ç”¨å•çº¿ç¨‹ä¼˜åŒ–å¤„ç†...")
    
    # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ‰¹é‡å¤„ç†
    for chunk in tqdm(segments, desc="é¢„åˆ†è¯è¿›åº¦", unit="æ®µè½"):
        # ä½¿ç”¨findallè€Œä¸æ˜¯finditerï¼Œå¯èƒ½æ›´å¿«
        tokens = pat.findall(chunk)
        for token in tokens:
            bs = token.encode("utf-8")
            freq[tuple(bs)] += 1
    
    return dict(freq)


def train_bpe_with_progress(input_path: str, vocab_size: int, special_tokens: list[str]):
    """
    å¸¦è¿›åº¦æ¡çš„BPEè®­ç»ƒå‡½æ•°
    """
    
    print("=== å¸¦è¿›åº¦æ¡çš„BPEè®­ç»ƒ ===\n")
    
    # 1. è¯»å–æ–‡ä»¶
    print("ğŸ“– è¯»å–è®­ç»ƒæ•°æ®...")
    with open(input_path, "r", encoding="utf-8") as f:
        text = f.read()
    
    file_size = len(text)
    print(f"   æ–‡ä»¶å¤§å°: {file_size:,} å­—ç¬¦")
    
    # 2. åˆ†å‰²ç‰¹æ®Štoken
    print("ğŸ”§ å¤„ç†ç‰¹æ®Štoken...")
    # å°†æ–‡æ¡£åˆ†å‰²æˆç‹¬ç«‹çš„æ®µè½ï¼Œæ¯ä¸ªæ®µè½åŒ…å«ä¸€ä¸ªç‰¹æ®Štoken
    # è¾“å…¥"Hello world!<|endoftext|>This is another document.<|endoftext|>Final document here."
    # è¾“å‡º["Hello world!","This is another document.","Final document here."]
    if not special_tokens:
        segments = [text]
    else:
        escaped = [re.escape(tok) for tok in special_tokens]
        pattern = re.compile("(" + "|".join(escaped) + ")")
        parts = pattern.split(text)
        segments = [seg for seg in parts if seg and seg not in special_tokens]
    
    print(f"   åˆ†å‰²æˆ {len(segments)} ä¸ªæ®µè½")
    
    # 3. é¢„åˆ†è¯ï¼ˆä¼˜åŒ–ç‰ˆæœ¬é€‰æ‹©ï¼‰
    print("âœ‚ï¸  é¢„åˆ†è¯...")
    
    # æ ¹æ®æ•°æ®é‡é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
    if len(segments) < 10000:  # å°æ•°æ®é›†ç”¨å•çº¿ç¨‹
        freq = _pretokenize_single_thread_optimized(segments)
    else:  # å¤§æ•°æ®é›†ç”¨å¤šè¿›ç¨‹ å¤šè¿›ç¨‹ä¸­é€šè¿‡åˆ†batchçš„å½¢å¼æ¥è·å–è¾ƒå¿«çš„é€Ÿåº¦
        freq = _pretokenize_parallel_optimized(segments)
    
    print(f"   é¢„åˆ†è¯å®Œæˆï¼Œå…± {len(freq):,} ä¸ªå”¯ä¸€token")
    
    # 4. åˆå§‹åŒ–è¯æ±‡è¡¨
    print("ğŸ“š åˆå§‹åŒ–è¯æ±‡è¡¨...")
    id_to_bytes = {}
    bytes_to_id = {}
    
    # 256å­—èŠ‚è¯æ±‡è¡¨
    for b in range(256):
        bb = bytes([b])
        id_to_bytes[b] = bb
        bytes_to_id[bb] = b
    
    next_id = 256
    # ç‰¹æ®Štoken
    for tok in special_tokens:
        b = tok.encode("utf-8")
        id_to_bytes[next_id] = b
        bytes_to_id[b] = next_id
        next_id += 1
    
    print(f"   åˆå§‹è¯æ±‡è¡¨å¤§å°: {len(id_to_bytes)}")
    
    # 5. å‡†å¤‡è¯­æ–™åº“
    print("ğŸ“ å‡†å¤‡è¯­æ–™åº“...")
    # æ¯ä¸€ä¸ªwordå¯¹åº”çš„IDåºåˆ—
    corpus_ids = []
    # æ¯ä¸€ä¸ªwordçš„é¢‘ç‡
    corpus_counts = []
    
    for byte_tuple, count in tqdm(freq.items(), desc="è½¬æ¢è¯­æ–™åº“", unit="token"):
        ids = [bytes_to_id[bytes([b])] for b in byte_tuple]
        corpus_ids.append(ids)
        corpus_counts.append(count)
    
    print(f"   è¯­æ–™åº“åŒ…å« {len(corpus_ids):,} ä¸ªtokenåºåˆ—")
    
    # 6. åˆå§‹åŒ–pairè®¡æ•°
    print("ğŸ”¢ åˆå§‹åŒ–å­—èŠ‚å¯¹è®¡æ•°...")
    # è¿™é‡Œå¼€å§‹å°±æ˜¯å¼€å§‹è®¡ç®—pairäº†ï¼Œè¿™é‡Œè®¡ç®—çš„pairæ˜¯æ¯ä¸ªtokenåºåˆ—å†…éƒ¨çš„pair
    
    def pairs_for_ids(ids):
        ctr = Counter()
        if len(ids) < 2:
            return ctr
        for a, b in zip(ids, ids[1:]):
            ctr[(a, b)] += 1
        return ctr
    
    # å­˜å‚¨æ¯ä¸ªè¯çš„å­—èŠ‚å¯¹è®¡æ•°
    # æ¯ä¸€ä¸ªelementæ˜¯Counterå¯¹è±¡ï¼Œkeyæ˜¯pairï¼Œvalueæ˜¯é¢‘ç‡ï¼Œå¯¹åº”è¿™ä¸ªwordä¸­æ‰€æœ‰pairçš„é¢‘ç‡
    word_pair_counters = []
    # å…¨å±€å­—èŠ‚å¯¹é¢‘ç‡ç»Ÿè®¡ é€šè¿‡wordçš„é¢‘ç‡æ¥åšä¸€ä¸ªè½¬æ¢
    # keyæ˜¯pairï¼Œvalueæ˜¯é¢‘ç‡
    total_pair_counts = defaultdict(int)
    # å­—èŠ‚å¯¹åˆ°åŒ…å«å®ƒçš„è¯çš„æ˜ å°„
    # keyæ˜¯pairï¼Œvalueæ˜¯åŒ…å«å®ƒçš„è¯çš„é›†åˆ
    pair_to_words = defaultdict(set)
    
    for i, ids in enumerate(tqdm(corpus_ids, desc="è®¡ç®—å­—èŠ‚å¯¹", unit="åºåˆ—")):
        ctr = pairs_for_ids(ids)
        word_pair_counters.append(ctr)

        # è¿™ä¸ªè¯çš„é¢‘ç‡
        c = corpus_counts[i]
        for pair, k in ctr.items():
            # è¿™ä¸ªå­—èŠ‚å¯¹çš„é¢‘ç‡ = è¿™ä¸ªwordçš„é¢‘ç‡ * è¿™ä¸ªwordä¸­è¿™ä¸ªå­—èŠ‚å¯¹çš„é¢‘ç‡
            total_pair_counts[pair] += k * c
            # è¿™ä¸ªå­—èŠ‚å¯¹åŒ…å«è¿™ä¸ªè¯ï¼Œå»ºç«‹æ˜ å°„å…³ç³»
            pair_to_words[pair].add(i)
    
    print(f"   å‘ç° {len(total_pair_counts):,} ä¸ªå”¯ä¸€å­—èŠ‚å¯¹")
    
    # 7. ä¸»åˆå¹¶å¾ªç¯
    merges = []
    max_merges = max(0, vocab_size - len(id_to_bytes))
    
    print(f"ğŸ”„ å¼€å§‹åˆå¹¶å¾ªç¯ï¼Œç›®æ ‡: {max_merges:,} æ¬¡åˆå¹¶")
    # ä¸€æ¬¡mergeé€ å°±ä¸€ä¸ªæ–°çš„word id

    def merge_pair(a, b, new_token):
        target = (a, b)
        # æ‰¾åˆ°åŒ…å«è¿™ä¸ªpairçš„æ‰€æœ‰çš„word
        affected_words = list(pair_to_words.get(target, set()))
        
        # éå†æ‰€æœ‰çš„words
        for i in affected_words:

            # æ‰¾åˆ°è¿™ä¸ªwordså¯¹åº”çš„IDåºåˆ— æ¯”å¦‚ hello â€”> [104, 101, 108]
            ids = corpus_ids[i]
            if len(ids) < 2:
                continue
            
            # ç§»é™¤æ—§çš„pairè®¡æ•°
            # æ‹¿å‡ºä¹‹å‰è¿™ä¸ªwordæ‰€åŒ…å«çš„æ‰€æœ‰çš„pairs
            old_pairs = word_pair_counters[i]

            # æ‹¿åˆ°è¿™ä¸ªwordçš„é¢‘ç‡æ¬¡æ•°
            count_multiplier = corpus_counts[i]

            # éå†è¿™ä¸ªwordçš„æ‰€æœ‰çš„pairs
            for pair, k in old_pairs.items():
                # æ›´æ–°å…¨å±€å­—èŠ‚å¯¹é¢‘ç‡ç»Ÿè®¡
                total_pair_counts[pair] -= k * count_multiplier
                # å¦‚æœè¿™ä¸ªpairçš„é¢‘ç‡å°äºç­‰äº0ï¼Œåˆ™ä»å…¨å±€å­—å…¸ä¸­åˆ é™¤è¿™ä¸ªpair
                if total_pair_counts[pair] <= 0:
                    total_pair_counts.pop(pair, None)
            
            # æ‰§è¡Œåˆå¹¶

            # å½“å‰å¤„ç†çš„ä½ç½®
            j = 0
            # æ–°çš„IDåºåˆ—
            out = []
            while j < len(ids):
                # å¦‚æœå½“å‰ä½ç½®çš„IDå’Œè¦åˆå¹¶çš„IDç›¸åŒï¼Œåˆ™æ›¿æ¢ä¸ºæ–°çš„ID
                # åœ¨è¿™é‡ŒåµŒå…¥æ–°çš„IDï¼Œ ç„¶åè·³è¿‡ä¸¤ä¸ªä½ç½®
                if j < len(ids) - 1 and ids[j] == a and ids[j + 1] == b:
                    out.append(new_token)
                    j += 2
                else:
                    # å¦åˆ™ï¼Œç›´æ¥æ·»åŠ å½“å‰ä½ç½®çš„åŸæ¥çš„ID
                    out.append(ids[j])
                    j += 1

            # æ›´æ–°è¿™ä¸ªwordçš„IDåºåˆ—
            corpus_ids[i] = out
            
            # é‡æ–°è®¡ç®—pairs
            new_ctr = pairs_for_ids(out)
            word_pair_counters[i] = new_ctr
            
            # æ›´æ–°ç´¢å¼•
            for pair in old_pairs.keys():
                # å¦‚æœè¿™ä¸ªpairä¸åœ¨æ–°çš„IDåºåˆ—ä¸­
                if pair not in new_ctr:
                    # æ‹¿åˆ°è¿™ä¸ªpairåŒ…å«çš„æ‰€æœ‰çš„words
                    s = pair_to_words.get(pair)
                    if s is not None:
                        # ä»è¿™ä¸ªpairåŒ…å«çš„æ‰€æœ‰çš„wordsä¸­åˆ é™¤è¿™ä¸ªword
                        s.discard(i)
                        if not s: # å¦‚æœæ²¡æœ‰è¯åŒ…å«è¿™ä¸ªå­—èŠ‚å¯¹äº†
                            # ä»å…¨å±€å­—å…¸ä¸­åˆ é™¤è¿™ä¸ªpair
                            pair_to_words.pop(pair, None)
            
            # æ›´æ–°æ–°çš„éƒ¨åˆ†
            for pair, k in new_ctr.items():
                # è¿™ä¸ªå­—èŠ‚å¯¹ç°æœ‰çš„æ¬¡æ•° + è¿™ä¸ªè¯è´¡çŒ®çš„æ¬¡æ•°
                total_pair_counts[pair] = total_pair_counts.get(pair, 0) + k * count_multiplier
                # æ›´æ–°pairåˆ°word
                s = pair_to_words.get(pair)
                if s is None:
                    pair_to_words[pair] = {i}
                else:
                    s.add(i)

    
    # mergeçš„æœ¬è´¨ç›®æ ‡å°±æ˜¯å»æ›´æ–°æ‰€æœ‰çš„ä¹‹å‰çš„å­˜å‚¨
    def merge_pair_debug(a, b, new_token):
        nonlocal total_pair_counts, pair_to_words, corpus_ids, word_pair_counters
        global merge_step_counter
        merge_step_counter += 1
        DEBUG = True
        
        # è¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ‰“å°å¯è¯»çš„pair
        def pretty_print_pair(pair):
            id1, id2 = pair
            # ä½¿ç”¨ .get é¿å…åœ¨æŸ¥æ‰¾ç‰¹æ®Š token æ—¶å‡ºé”™
            byte1 = id_to_bytes.get(id1, b'??')
            byte2 = id_to_bytes.get(id2, b'??')
            return f"({repr(byte1)}, {repr(byte2)})"

        initial_len = len(total_pair_counts)
        # --- å®¡è®¡ï¼šè®°å½•åˆå¹¶å‰çš„æ‰€æœ‰ unique pairs ---
        initial_pairs_set = set(total_pair_counts.keys())

        if DEBUG and merge_step_counter <= 10:
            print(f"\n" + "="*80)
            print(f"[DEBUG] --- MERGE STEP {merge_step_counter} ---")
            print(f"[DEBUG] Merging pair: {pretty_print_pair((a,b))} -> ID {new_token}")
            print(f"[DEBUG] Initial len(total_pair_counts): {initial_len}")

        affected_words_indices = list(pair_to_words.get((a, b), set()))
        
        # --- é˜¶æ®µä¸€ï¼šæ›´æ–°æ‰€æœ‰å—å½±å“çš„è¯çš„IDåºåˆ— ---
        for i in affected_words_indices:
            ids = corpus_ids[i]
            new_ids = []
            j = 0
            while j < len(ids):
                if j < len(ids) - 1 and ids[j] == a and ids[j+1] == b:
                    new_ids.append(new_token)
                    j += 2
                else:
                    new_ids.append(ids[j])
                    j += 1
            corpus_ids[i] = new_ids
            # æ³¨æ„ï¼šæˆ‘ä»¬åªæ›´æ–° corpus_idsï¼Œper-word counters ä¼šåœ¨é‡å»ºæ—¶è‡ªåŠ¨æ›´æ–°

        # --- é˜¶æ®µäºŒï¼šå®Œå…¨åŸºäºæ›´æ–°åçš„çŠ¶æ€ï¼Œé‡å»ºå…¨å±€è®¡æ•°å’Œç´¢å¼• ---
        total_pair_counts = defaultdict(int)
        pair_to_words = defaultdict(set)
        # æ³¨æ„ï¼šè¿™é‡Œçš„ word_pair_counters ä¹Ÿéœ€è¦é‡æ–°è®¡ç®—
        for i in range(len(corpus_ids)):
            # é‡æ–°è®¡ç®— per-word counter
            ctr = pairs_for_ids(corpus_ids[i])
            word_pair_counters[i] = ctr
            
            # ç´¯åŠ åˆ°å…¨å±€è®¡æ•°
            c = corpus_counts[i]
            for pair, k in ctr.items():
                total_pair_counts[pair] += k * c
                pair_to_words[pair].add(i)

        final_len = len(total_pair_counts)
        
        # --- å®¡è®¡ï¼šæ¯”è¾ƒåˆå¹¶å‰åçš„ unique pairs é›†åˆ ---
        final_pairs_set = set(total_pair_counts.keys())
        
        removed_pairs = initial_pairs_set - final_pairs_set
        added_pairs = final_pairs_set - initial_pairs_set

        if DEBUG and merge_step_counter <= 10:
            # å°† set è½¬æ¢ä¸º list å¹¶æ’åºï¼Œè®©è¾“å‡ºæ›´ç¨³å®š
            removed_list = sorted(list(removed_pairs))
            added_list = sorted(list(added_pairs))

            print(f"    (-) Pairs REMOVED ({len(removed_list)} unique types): {{ {', '.join(pretty_print_pair(p) for p in removed_list)} }}")
            print(f"    (+) Pairs ADDED   ({len(added_list)} unique types): {{ {', '.join(pretty_print_pair(p) for p in added_list)} }}")
            
            print(f"[DEBUG] Final len(total_pair_counts): {final_len}")
            print(f"[DEBUG] Net change in len for this step: {final_len - initial_len}  ({len(added_list)} added - {len(removed_list)} removed)")
            print("="*80)
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºåˆå¹¶è¿›åº¦
    with tqdm(total=max_merges, desc="åˆå¹¶è¿›åº¦", unit="åˆå¹¶") as pbar:
        for merge_step in range(max_merges):
            if not total_pair_counts:
                pbar.set_description("åˆå¹¶å®Œæˆ (æ— æ›´å¤šå­—èŠ‚å¯¹)")
                break
            
            # é€‰æ‹©æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹
            # è¿™æ ·å¯ä»¥ä¿è¯å­—å…¸åºæœ€å°çš„pairè¢«é€‰ä¸­ï¼ŒåŒæ—¶æ¯”è¾ƒçš„æ˜¯å®é™…å­—èŠ‚çš„å†…å®¹ï¼Œè€Œä¸æ˜¯IDæ•°å€¼
            def pair_key(p):
                a, b = p
                return (total_pair_counts[p], id_to_bytes[a], id_to_bytes[b])
            
            best_pair = max(total_pair_counts.keys(), key=pair_key)
            a, b = best_pair
            
            # åˆ›å»ºæ–°token
            bytes_a = id_to_bytes[a]
            bytes_b = id_to_bytes[b]
            # åˆ›å»ºæ–°bytesï¼Œè¿™é‡Œå°±æ˜¯åˆå¹¶ä¸¤ä¸ªbytes
            new_bytes = bytes_a + bytes_b
            new_id = next_id
            next_id += 1
            # æ›´æ–°è¯æ±‡è¡¨
            id_to_bytes[new_id] = new_bytes
            
            # æ‰§è¡Œåˆå¹¶,æ›´æ–°ä¹‹å‰è®¾è®¡çš„æ‰€æœ‰çš„æ•°æ®ç»“æ„
            merge_pair(a, b, new_id)
            # è®°å½•merge
            merges.append((bytes_a, bytes_b))
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'å½“å‰åˆå¹¶': f"{bytes_a} + {bytes_b}",
                'è¯æ±‡è¡¨å¤§å°': len(id_to_bytes),
                'å‰©ä½™å­—èŠ‚å¯¹': len(total_pair_counts)
            })
            pbar.update(1)
            
            if len(id_to_bytes) >= vocab_size:
                pbar.set_description("åˆå¹¶å®Œæˆ (è¾¾åˆ°ç›®æ ‡è¯æ±‡è¡¨å¤§å°)")
                break
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"   æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(id_to_bytes):,}")
    print(f"   åˆå¹¶è§„åˆ™æ•°é‡: {len(merges):,}")
    
    return id_to_bytes, merges


def detect_dataset(input_path: str) -> str:
    """æ£€æµ‹æ•°æ®é›†ç±»å‹"""
    path = Path(input_path)
    if "tinystories" in path.name.lower():
        return "tinystories"
    elif "owt" in path.name.lower() or "openwebtext" in path.name.lower():
        return "owt"
    else:
        return "unknown"


def get_dataset_config(dataset_type: str) -> dict:
    """è·å–æ•°æ®é›†é…ç½®"""
    configs = {
        "tinystories": {
            "vocab_size": 10000,
            "special_tokens": ["<|endoftext|>"],
            "description": "TinyStoriesæ•°æ®é›†"
        },
        "owt": {
            "vocab_size": 32000,
            "special_tokens": ["<|endoftext|>"],
            "description": "OpenWebTextæ•°æ®é›†"
        },
        "unknown": {
            "vocab_size": 10000,
            "special_tokens": ["<|endoftext|>"],
            "description": "æœªçŸ¥æ•°æ®é›†"
        }
    }
    return configs.get(dataset_type, configs["unknown"])


def train_bpe_with_dataset(input_path: str, dataset_type: str = None, vocab_size: int = None, special_tokens: list[str] = None):
    """
    åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šè®­ç»ƒBPEåˆ†è¯å™¨ï¼ˆå¸¦è¿›åº¦æ¡ç‰ˆæœ¬ï¼‰
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        dataset_type: æ•°æ®é›†ç±»å‹ ("tinystories", "owt", "auto")
        vocab_size: è¯æ±‡è¡¨å¤§å°ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
        special_tokens: ç‰¹æ®Štokenåˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
    """
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç±»å‹
    if dataset_type is None or dataset_type == "auto":
        dataset_type = detect_dataset(input_path)
    
    # è·å–æ•°æ®é›†é…ç½®
    config = get_dataset_config(dataset_type)
    
    # ä½¿ç”¨ç”¨æˆ·æä¾›çš„å‚æ•°æˆ–é»˜è®¤é…ç½®
    if vocab_size is None:
        vocab_size = config["vocab_size"]
    if special_tokens is None:
        special_tokens = config["special_tokens"]
    
    print(f"=== è®­ç»ƒBPEåˆ†è¯å™¨åœ¨{config['description']}ä¸Šï¼ˆå¸¦è¿›åº¦æ¡ï¼‰ ===\n")
    print(f"æ£€æµ‹åˆ°çš„æ•°æ®é›†ç±»å‹: {dataset_type}")
    
    # é…ç½®å‚æ•°
    print(f"é…ç½®å‚æ•°:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  ç‰¹æ®Štoken: {special_tokens}")
    print(f"  è¾“å…¥æ–‡ä»¶: {input_path}")
    print()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(input_path).exists():
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {input_path}")
        return None
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    try:
        # è®­ç»ƒBPEåˆ†è¯å™¨
        vocab, merges = train_bpe_with_progress(
            input_path=input_path,
            vocab_size=vocab_size,
            special_tokens=special_tokens
        )
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        training_time = end_time - start_time
        
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’ ({training_time/60:.2f} åˆ†é’Ÿ)")
        print(f"   æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        print(f"   åˆå¹¶è§„åˆ™æ•°é‡: {len(merges)}")
        
        # åˆ†æè¯æ±‡è¡¨
        print(f"\nğŸ” è¯æ±‡è¡¨åˆ†æ:")
        
        # æ‰¾å‡ºæœ€é•¿çš„token
        longest_token = max(vocab.values(), key=len)
        print(f"   æœ€é•¿token: {repr(longest_token)} (é•¿åº¦: {len(longest_token)})")
        
        try:
            longest_str = longest_token.decode('utf-8', errors='replace')
            print(f"   æœ€é•¿tokenè§£ç : '{longest_str}'")
        except:
            print(f"   æœ€é•¿tokenæ— æ³•è§£ç ä¸ºUTF-8")
        
        # ç»Ÿè®¡ä¸åŒé•¿åº¦çš„token
        length_counts = {}
        for token_bytes in vocab.values():
            length = len(token_bytes)
            length_counts[length] = length_counts.get(length, 0) + 1
        
        print(f"\n   Tokené•¿åº¦åˆ†å¸ƒ:")
        for length in sorted(length_counts.keys()):
            print(f"     é•¿åº¦ {length}: {length_counts[length]:,} ä¸ªtoken")
        
        # æ˜¾ç¤ºä¸€äº›å­¦ä¹ åˆ°çš„token
        learned_tokens = {k: v for k, v in vocab.items() if k >= 256}
        print(f"\n   ğŸ“ å­¦ä¹ åˆ°çš„tokenç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰:")
        for i, (token_id, token_bytes) in enumerate(sorted(learned_tokens.items())[:10]):
            try:
                token_str = token_bytes.decode('utf-8', errors='replace')
                print(f"     {token_id}: {repr(token_bytes)} -> '{token_str}'")
            except:
                print(f"     {token_id}: {repr(token_bytes)}")
        
        # åºåˆ—åŒ–ç»“æœåˆ°ç£ç›˜
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœ...")
        output_dir = Path("bpe_results")
        output_dir.mkdir(exist_ok=True)
        
        # æ ¹æ®æ•°æ®é›†ç±»å‹ç”Ÿæˆæ–‡ä»¶å
        dataset_prefix = dataset_type if dataset_type != "unknown" else "custom"
        
        # ä¿å­˜è¯æ±‡è¡¨ï¼ˆJSONæ ¼å¼ï¼‰
        vocab_json = {str(k): list(v) for k, v in vocab.items()}
        vocab_file = output_dir / f"{dataset_prefix}_vocab.json"
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump(vocab_json, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜åˆå¹¶è§„åˆ™ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
        merges_file = output_dir / f"{dataset_prefix}_merges.txt"
        with open(merges_file, 'w', encoding='utf-8') as f:
            for left, right in merges:
                f.write(f"{left} {right}\n")
        
        print(f"   ç»“æœå·²ä¿å­˜åˆ°:")
        print(f"     ğŸ“„ è¯æ±‡è¡¨: {vocab_file}")
        print(f"     ğŸ“„ åˆå¹¶è§„åˆ™: {merges_file}")
        
        # éªŒè¯ç»“æœ
        print(f"\nâœ… éªŒè¯ç»“æœ:")
        special_token_bytes = '<|endoftext|>'.encode('utf-8')
        has_special_token = any(token == special_token_bytes for token in vocab.values())
        print(f"   ç‰¹æ®Štoken <|endoftext|> åœ¨è¯æ±‡è¡¨ä¸­: {has_special_token}")
        print(f"   è¯æ±‡è¡¨å¤§å°ç¬¦åˆè¦æ±‚: {len(vocab) == vocab_size}")
        print(f"   åˆå¹¶è§„åˆ™æ•°é‡: {len(merges)}")
        
        return {
            'vocab': vocab,
            'merges': merges,
            'training_time': training_time,
            'vocab_size': vocab_size,
            'special_tokens': special_tokens,
            'dataset_type': dataset_type
        }
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


def train_bpe_tinystories_with_progress():
    """
    åœ¨TinyStoriesæ•°æ®é›†ä¸Šè®­ç»ƒBPEåˆ†è¯å™¨ï¼ˆå¸¦è¿›åº¦æ¡ç‰ˆæœ¬ï¼‰
    """
    input_path = "../../data/TinyStoriesV2-GPT4-train.txt"
    return train_bpe_with_dataset(input_path, dataset_type="tinystories")


def train_bpe_owt_with_progress():
    """
    åœ¨OpenWebTextæ•°æ®é›†ä¸Šè®­ç»ƒBPEåˆ†è¯å™¨ï¼ˆå¸¦è¿›åº¦æ¡ç‰ˆæœ¬ï¼‰
    """
    input_path = "../../data/owt_train.txt"  # å‡è®¾çš„OWTè·¯å¾„
    return train_bpe_with_dataset(input_path, dataset_type="owt")


if __name__ == "__main__":
    import sys
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        dataset_type = sys.argv[1].lower()
        if dataset_type == "tinystories":
            results = train_bpe_tinystories_with_progress()
        elif dataset_type == "owt":
            results = train_bpe_owt_with_progress()
        elif dataset_type == "auto":
            # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†
            input_path = sys.argv[2] if len(sys.argv) > 2 else "./data/TinyStoriesV2-GPT4-train.txt"
            results = train_bpe_with_dataset(input_path, dataset_type="auto")
        else:
            # è‡ªå®šä¹‰æ•°æ®é›†
            input_path = sys.argv[1]
            vocab_size = int(sys.argv[2]) if len(sys.argv) > 2 else None
            results = train_bpe_with_dataset(input_path, vocab_size=vocab_size)
    else:
        # é»˜è®¤è®­ç»ƒTinyStories
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python train_bpe_with_progress.py tinystories")
        print("  python train_bpe_with_progress.py owt")
        print("  python train_bpe_with_progress.py auto <input_path>")
        print("  python train_bpe_with_progress.py <input_path> [vocab_size]")
        print()
        print("é»˜è®¤è®­ç»ƒTinyStoriesæ•°æ®é›†...")
        results = train_bpe_tinystories_with_progress()
    
    if results:
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    else:
        print("\nğŸ’¥ è®­ç»ƒå¤±è´¥ï¼")
